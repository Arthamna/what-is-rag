{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "%pip install -Uq \"unstructured[all-docs]\" \n",
    "%pip install -Uq langchain_chroma \n",
    "%pip install -Uq langchain langchain-community langchain-openai \n",
    "%pip install -Uq python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "# Unstructured for document parsing\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./\"\n",
    "\n",
    "def partition_pdf_elements(file_path):\n",
    "    elements = partition_pdf(\n",
    "            filename=file_path,  # Path to your PDF file\n",
    "            strategy=\"hi_res\", # Use the most accurate (but slower) processing method of extraction\n",
    "            infer_table_structure=True, # Keep tables as structured HTML, not jumbled text\n",
    "            extract_image_block_types=[\"Image\"], # Grab images found in the PDF\n",
    "            extract_image_block_to_payload=True # Store images as base64 data you can actually use\n",
    "        )\n",
    "    print(f\"Found {len(elements)} elements\")\n",
    "    return elements\n",
    "\n",
    "elements = partition_pdf_elements(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All types of different atomic elements we see from unstructured library\n",
    "set([str(type(el)) for el in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check content in array\n",
    "elements[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect e.g : img in elements\n",
    "images = [element for element in elements if element.category == 'Image']\n",
    "print(f\"Found {len(images)} images\")\n",
    "\n",
    "images[0].to_dict()\n",
    "# Use https://codebeautify.org/base64-to-image-converter to view the base64 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_title(elements):\n",
    "    chunks = chunk_by_title(\n",
    "        elements, # The parsed PDF elements from previous step\n",
    "        max_characters=3000, # Hard limit - never exceed 3000 characters per chunk\n",
    "        new_after_n_chars=2400, # Try to start a new chunk after 2400 characters\n",
    "        combine_text_under_n_chars=500 # Merge tiny chunks under 500 chars with neighbors\n",
    "    )   \n",
    "\n",
    "    print(f\"Make {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "chunks = create_chunks_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All unique chunk types\n",
    "set([str(type(chunk)) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a single chunk\n",
    "# chunks[2].to_dict()\n",
    "\n",
    "# View original elements\n",
    "chunks[11].metadata.orig_elements[-1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_content_types(chunk):\n",
    "    content_data = {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'types': ['text']\n",
    "    }\n",
    "\n",
    "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            element_type = type(element).__name__\n",
    "            \n",
    "            # Handle tables\n",
    "            if element_type == 'Table':\n",
    "                content_data['types'].append('table')\n",
    "                table_html = getattr(element.metadata, 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "            \n",
    "            # Handle images\n",
    "            elif element_type == 'Image':\n",
    "                if hasattr(element, 'metadata') and hasattr(element.metadata, 'image_base64'):\n",
    "                    content_data['types'].append('image')\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "    \n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data\n",
    "\n",
    "separate_content_types(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ai_summary(text: str, tables: List[str], images: List[str]):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    # Build the text prompt\n",
    "    prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "\n",
    "    CONTENT TO ANALYZE:\n",
    "    TEXT CONTENT:\n",
    "    {text}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add tables if present\n",
    "    if tables:\n",
    "        prompt_text += \"TABLES:\\n\"\n",
    "        for i, table in enumerate(tables):\n",
    "            prompt_text += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "    \n",
    "            prompt_text += \"\"\"\n",
    "            YOUR TASK:\n",
    "            Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "            1. Key facts, numbers, and data points from text and tables\n",
    "            2. Main topics and concepts discussed  \n",
    "            3. Questions this content could answer\n",
    "            4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "            5. Alternative search terms users might use\n",
    "\n",
    "            Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "            SEARCHABLE DESCRIPTION:\"\"\"\n",
    "\n",
    "    # Build message content starting with text\n",
    "    message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "    \n",
    "    # Add images to the message\n",
    "    for image_base64 in images:\n",
    "        message_content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "        })\n",
    "    \n",
    "    # Send to AI and get response\n",
    "    message = HumanMessage(content=message_content)\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(chunks):\n",
    "    langchain_documents = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        content = separate_content_types(chunk)\n",
    "        summary = create_ai_summary(content['text'], content['tables'], content['images'])\n",
    "\n",
    "        doc = Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": summary['text'],\n",
    "                    \"tables_html\": summary['tables'],\n",
    "                    \"images_base64\": summary['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "\n",
    "        langchain_documents.append(doc)\n",
    "    return langchain_documents\n",
    "\n",
    "proccesed_chunks = create_summary(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others\n",
    "def export_chunks_to_json(chunks, output=\"chunks.json\"):\n",
    "    export_data = []\n",
    "    for i, doc in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"enhanced_content\": doc.page_content,\n",
    "            \"metadata\": {\n",
    "                \"original_content\": json.loads(doc.metadata.get(\"original_content\", \"{}\"))\n",
    "            } \n",
    "        }\n",
    "        export_data.append(chunk_data)\n",
    "        \n",
    "    with open(output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Exported {len(export_data)} chunks to {output}\")\n",
    "    return export_data\n",
    "    \n",
    "json_data = export_chunks_to_json(proccesed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks, directory=\"db/chroma_db\"):\n",
    "\n",
    "    # embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    embedding_model = HuggingFaceEndpointEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # docs = [Document(page_content=text) for text in docs]\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=directory,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"} # ?\n",
    "    )\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "db = create_vector_store(proccesed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example retrieval - chunks\n",
    "query = \"What are the two main components of the Transformer architecture? \"\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "# export_chunks_to_json(chunks, \"rag_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def complete_ingestion_pipeline(file_path):\n",
    "#     elements = partition_pdf_elements(file_path)\n",
    "#     chunks = create_chunks_title(elements)\n",
    "#     proccesed_chunks = create_summary(chunks)\n",
    "#     db = create_vector_store(proccesed_chunks)\n",
    "#     return db\n",
    "\n",
    "# db = complete_ingestion_pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple query - answer\n",
    "\n",
    "query = \"What are the two main components of the Transformer architecture? \"\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "\n",
    "def generate_answer(chunks):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    # Build the text prompt\n",
    "    prompt_text = f\"\"\"Based on the following documents, please answer this question: {query}\n",
    "\n",
    "    CONTENT TO ANALYZE:\n",
    "    \"\"\"\n",
    "        \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        prompt_text += f\"--- Document {i+1} ---\\n\"\n",
    "        \n",
    "        if \"original_content\" in chunk.metadata:\n",
    "            original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "            \n",
    "            # Add raw text\n",
    "            raw_text = original_data.get(\"raw_text\", \"\")\n",
    "            if raw_text:\n",
    "                prompt_text += f\"TEXT:\\n{raw_text}\\n\\n\"\n",
    "            \n",
    "            # Add tables as HTML\n",
    "            tables_html = original_data.get(\"tables_html\", [])\n",
    "            if tables_html:\n",
    "                prompt_text += \"TABLES:\\n\"\n",
    "                for j, table in enumerate(tables_html):\n",
    "                    prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "        \n",
    "        prompt_text += \"\\n\"\n",
    "    \n",
    "    prompt_text += \"\"\"\n",
    "        Please provide a clear, comprehensive answer using the text, tables, and images above. If the documents don't contain sufficient information to answer the question, say \"I don't have enough information to answer that question based on the provided documents.\"\n",
    "\n",
    "        ANSWER:\"\"\"\n",
    "\n",
    "    # Build message content starting with text\n",
    "    message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "    \n",
    "    # Add all images from all chunks\n",
    "    for chunk in chunks:\n",
    "        if \"original_content\" in chunk.metadata:\n",
    "            original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "            images_base64 = original_data.get(\"images_base64\", [])\n",
    "            \n",
    "            for image_base64 in images_base64:\n",
    "                message_content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "                })\n",
    "    \n",
    "    # Send to AI and get response\n",
    "    message = HumanMessage(content=message_content)\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "final_answer = generate_answer(chunks, query)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
